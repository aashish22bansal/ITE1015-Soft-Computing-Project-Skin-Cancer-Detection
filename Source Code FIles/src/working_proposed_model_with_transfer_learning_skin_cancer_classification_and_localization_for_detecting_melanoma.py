# -*- coding: utf-8 -*-
"""WORKING PROPOSED MODEL WITH TRANSFER LEARNING Skin cancer classification and localization for detecting melanoma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecKVcpLR-9esUjROo4IVH8PSi8arEzki

# <center><b>Skin Cancer Classification for Detecting Melanoma <br> Using <br> Transfer Learning and Ensemble Modeling</b></center>
<center>
<h2>J-Component - Soft Computing</h2>
<p>Made under the guidance of <b>Dr. Agilandeeswari L</b><br>
</center>
By:<br>
Aashish Bansal 19BIT0346<br>
Keerthi Yasasvi 19BIT03<br>
Perumalla Sasank 19BIT03<br>
</p>

The project is a Transfer Learning and CNN trained model which can predict whether the patient has a suffering from Cancer or not by checking the images of the infected areas on the body. The model has been trained on a variety of images through which it predicts the required.
In this project, the image file of the patient is upload into a software, which is GUI-based interface, developed with the help of Tkinter, and it consists of the model saved as a file and the software uses that to analyze the image and give the prediction which can help doctors to start with the medication way faster instead of waiting for the laboratory reports for the confirmation.
So basically,
* Skin cancer is an abnormal growth of skin cells. Most skin cancers are caused by exposure to ultraviolet (UV) light. When the skin is not protected, UV rays from sunlight or tanning beds can damage and alter skin's DNA that leads to the cancer.
* Deep learning model has been built to classify and identify the binary diagnostic group of melanocytic images obtained through dermoscopy.
* Based on the model, disease detection through dermal cell images has been investigated, and classifications on dermal cell images have been performed.

Ensembles are predictive models that combine predictions from two or more other models. Ensemble learning methods are popular and the goto technique when the best performance on a predictive modeling project is the most important outcome. Nevertheless, they are not always the most appropriate technique to use and beginners the field of applied machine learning have the expectation that ensembles or a specific ensemble method are always the best method to use. Ensembles offer two specific benefits on a predictive modeling project, and it is important to know what these benefits are and how to measure them to ensure that using an ensemble is the right decision on your project. In this tutorial, you will discover the benefits of using ensemble methods for machine learning. After reading this tutorial, you will know:
*	A minimum benefit of using ensembles is to reduce the spread in the average skill of a predictive model.
*	A key benefit of using ensembles is to improve the average prediction performance over any contributing member in the ensemble.
*	The mechanism for improved performance with ensembles is often the reduction in the variance component of prediction errors made by the contributing models.

## 1. Preprocessing

### 1.1. Loading Libraries

#### 1.1.1. Importing Libraries
"""

import numpy as np 
import pandas as pd
import os
import cv2
import csv

import matplotlib.pyplot as plt
# import matplotlib.pylab as plt

from sklearn.datasets import load_files

import keras
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
from keras.utils import np_utils
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Flatten, BatchNormalization, Activation, Dropout
from keras.callbacks import ModelCheckpoint, TensorBoard

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.python.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D,Activation,BatchNormalization,Dropout
from tensorflow.python.keras import Sequential,backend,optimizers

from numpy import *

from tkinter import *

from PIL import ImageTk
from PIL import ImageFile
from PIL import Image

from tqdm import tqdm

from glob import glob

ImageFile.LOAD_TRUNCATED_IMAGES = True

"""### 1.2. Loading Data

#### 1.2.1. Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### 1.2.2. Training Data

##### 1.2.2.1. Importing Training Data
"""

# Load text files with categories as subfolder names.
path_training_data = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Part3_GroundTruth.csv"
training_data = pd.read_csv(path_training_data)

"""##### 1.2.2.2 Display the Training Data of the CSV File"""

training_data

print("Filename: \n", training_data['image_id'][:5])

print("Targets: \n", training_data['melanoma'][:5])

"""##### 1.2.2.3 Obtaining the Labels of all the Images as One-Hot Encoding"""

# Getting the labels
target = np_utils.to_categorical(np.array(training_data['melanoma']), 2)
target

"""##### 1.2.2.4 Checking the Number of Training Images"""

len(training_data['image_id'])

"""##### 1.2.2.5 Loading the Image Filenames"""

# Splitting the data into the training and validation set
#load_dataset('/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Data/Data Images JPG/')
train_files, train_targets = training_data['image_id'][:2000], target[:2000]

train_files

train_targets

"""#### 1.2.3 Validation Data

##### 1.2.3.1. Importing Validation Dataset
"""

# Load text files with categories as subfolder names.
path_validation_data = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Part3_GroundTruth.csv"
validation_data = pd.read_csv(path_validation_data)

"""##### 1.2.3.2 Display the Validation Data of the CSV File"""

validation_data

print("Filename: \n", validation_data['image_id'][:5])

print("Targets: \n", validation_data['melanoma'][:5])

"""##### 1.2.3.3 Obtaining the Labels of all the Images as One-Hot Encoding"""

# Getting the labels
target = np_utils.to_categorical(np.array(validation_data['melanoma']), 2)
target

"""##### 1.2.3.4. Checking the Number of Validation Images"""

len(validation_data['image_id'])

"""##### 1.2.3.5 Loading the Images Filenames of the Validation Data"""

# train_files, train_targets = training_data['image_id'][:2000], target[:2000]
valid_files, valid_targets = validation_data['image_id'][:150], target[:150]

valid_files

valid_targets

"""#### 1.2.4. Testing Data

##### 1.2.4.1.  Importing Testing Dataset
"""

# Load text files with categories as subfolder names.
path_testing_data = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Test_v2_Part3_GroundTruth.csv"
testing_data = pd.read_csv(path_testing_data)

"""##### 1.2.4.2. Display the Testing Data of the CSV File"""

testing_data

print("Filename: \n", testing_data['image_id'][:5])

print("Targets: \n", testing_data['melanoma'][:5])

"""##### 1.2.4.3. Obtaining the Labels of all the Images as One-Hot Encoding"""

# Getting the labels
test_target = np_utils.to_categorical(np.array(testing_data['melanoma']), 2)
test_target

"""##### 1.2.4.4. Checking the Number of Testing Images"""

len(testing_data['image_id'])

"""##### 1.2.4.5. Loading the Images Filenames of the Testing Data"""

# train_files, train_targets = training_data['image_id'][:2000], target[:2000]
test_files, test_targets = testing_data['image_id'][:600], test_target[:600]

test_files

len(test_targets)

test_targets

"""### 1.3. Image preprocessing"""

def path_to_tensor(img_path):
    """
    Getting a tensor from a given path.
    """
    # Loading the image
    img = image.load_img(img_path, target_size=(512, 512))
    # Converting the image to numpy array
    x = image.img_to_array(img)
    # convert 3D tensor to 4D tensor with shape (1, 512, 512, 3)
    return np.expand_dims(x, axis=0)

def paths_to_tensor(img_paths):
    """
    # Getting a list of tensors from a given path directory.
    """
    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]
    return np.vstack(list_of_tensors)

"""#### 1.3.1 Training Data

##### 1.3.1.1. Display the Filenames of Training Data
"""

train_files

"""##### 1.3.1.2. DIsplay all the Filenames present in the Dataset Directory"""

import os
os.chdir("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Data/Data Images JPG/")
!ls

"""##### 1.3.1.3 Joining the Filenames with the Directory Path of Every File"""

# pre-process the data for Keras
# Training Path: /content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Data/Data Images JPG
# os.path.join(folder, file)
dt = os.walk('/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Data/Data Images JPG')
files = []
for root, d_names, f_names in dt:
    for filename in f_names:
        files.append(os.path.join(root, filename))

files

"""##### 1.3.1.4. Converting the Complete Filenames to Tensors"""

train_tensors = paths_to_tensor(files).astype('float32')/255

"""#### 1.3.2. Validation Data

##### 1.3.2.1. Display the Filenames of Validation Data
"""

valid_files

"""##### 1.3.2.2. DIsplay all the Filenames present in the Dataset Directory"""

import os
os.chdir("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Data/Data Image JPG/")
!ls

"""##### 1.3.2.3 Joining the Filenames with the Directory Path of Every File"""

dt = os.walk('/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Data/Data Image JPG')
validation_files = []
for root, d_names, f_names in dt:
    for filename in f_names:
        validation_files.append(os.path.join(root, filename))

validation_files

"""##### 1.3.2.4. Converting the Complete Filenames to Tensors"""

valid_tensors = paths_to_tensor(validation_files).astype('float32')/255

"""#### 1.3.3. Test Data

##### 1.3.3.1. Display the Filenames of Testing Data
"""

test_files

"""##### 1.3.3.2. DIsplay all the Filenames present in the Dataset Directory"""

import os
os.chdir("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Test_v2_Data/Data Image JPG/")
!ls

"""##### 1.3.3.3 Joining the Filenames with the Directory Path of Every File"""

# pre-process the data for Keras
# Training Path: /content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Training_Data/Data Images JPG
# os.path.join(folder, file)
dt = os.walk('/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Test_v2_Data/Data Image JPG')
test_files = []
for root, d_names, f_names in dt:
    for filename in f_names:
        test_files.append(os.path.join(root, filename))

len(test_files)

test_files

"""##### 1.3.3.4. Converting the Complete Filenames to Tensors"""

test_tensors = paths_to_tensor(test_files).astype('float32')/255

"""### 1.4. Saving Tensor Files

#### 1.4.1 Saving Files into Drive
"""

# Saving the data
np.save("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_training_tensors.npy", train_tensors)
np.save("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_validation_tensors.npy", valid_tensors)
np.save("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_testing_tensors.npy", test_tensors)

"""#### 1.4.2 Loading the Tensor Files into Model"""

# Loading the data
train_tensors = np.load("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_training_tensors.npy")
valid_tensors = np.load("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_validation_tensors.npy")
test_tensors = np.load("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved image tensors/augmented_testing_tensors.npy")

"""## 2. Training the model

### 2.1. MobileNet Architecture

#### 2.1.1 Defining the MobileNet Architecture Function
"""

def mobilenet_architecture():
    """
    Pre-build architecture of mobilenet for our dataset.
    """
    # Imprting the model
    from keras.applications.mobilenet import MobileNet

    # Pre-build model
    base_model = MobileNet(include_top = False, weights = None, input_shape = (512, 512, 3))

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    mobilenet_model = Model(base_model.input, output)
    
    # Getting the summary of architecture
    mobilenet_model.summary()
    
    # Compiling the model
    mobilenet_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])

    return mobilenet_model

# Getting the mobilenet
mobilenet_model = mobilenet_architecture()

"""#### 2.1.2. Creating a Checkpoint for the Model"""

checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5', 
                               verbose=1, 
                               save_best_only=True)

"""#### 2.1.3. Fitting into the Model"""

mobilenet_model.fit(train_tensors, 
                    train_targets, 
                    batch_size = 8,
                    validation_data = (valid_tensors, valid_targets),
                    epochs = 5,
                    callbacks=[checkpointer], 
                    verbose=1)

"""#### 2.1.4. Loading the Weights for the MobileNet"""

# Loading the weights
mobilenet_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5")

"""### 2.2. Inception Architecture

#### 2.2.1 Defining the Inception Architecture Function
"""

def inception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model 
    from keras.applications.inception_v3 import InceptionV3

    # Pre-build model
    base_model = InceptionV3(include_top = False, weights = None, input_shape = (512, 512, 3))

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    inception_model = Model(base_model.input, output)
    
    # Summary of the model
    inception_model.summary()
    
    # Compiling the model
    inception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])
    
    return inception_model

# Getting the inception
inception_model = inception_architecture()

"""#### 2.2.2 Creating a Checkpoint for the Model"""

checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5', 
                               verbose=1, 
                               save_best_only=True)

"""#### 2.2.3 Fitting into the Model"""

inception_model.fit(train_tensors, 
                    train_targets, 
                    batch_size = 8,
                    validation_data = (valid_tensors, valid_targets),
                    epochs = 5,
                    callbacks=[checkpointer], 
                    verbose=1)

"""#### 2.2.4 Loading the Weights for the Inception"""

# Loading the weights
inception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5")

"""### 2.3 Xception architecture

#### 2.3.1 Defining the Xception Architechture
"""

def xception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model
    from keras.applications.xception import Xception

    # Pre-build model
    base_model = Xception(include_top = False, weights = None, input_shape = (512, 512, 3))

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    xception_model = Model(base_model.input, output)

    # Summary of the model
    xception_model.summary()
    
    # Compiling the model
    xception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                           loss = 'categorical_crossentropy', 
                           metrics = ['accuracy'])

    return xception_model

# Getting the xception
xception_model = xception_architecture()

"""#### 2.3.2 Creating a Checkpoint for the Model"""

tensor_board = TensorBoard(log_dir='./logs', histogram_freq = 0, batch_size = 8)

checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5', 
                               verbose=1, 
                               save_best_only=True)

"""#### 2.3.3 Fitting into the Model"""

xception_model.fit(train_tensors, 
                   train_targets, 
                   batch_size = 8,
                   validation_data = (valid_tensors, valid_targets),
                   epochs = 2,
                   callbacks=[checkpointer, tensor_board], 
                   verbose=1)

"""#### 2.3.4 Loading the Weights"""

# Loading the weights
xception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5")

"""## 3. Prediction

#### Declaring some Variables
"""

predict_0_0 = 0
predict_0_1 = 0

model_architecture = None
weight_path = ""

"""#### Defining the Prediction Function"""

def predict(img_path,
            model_architecture = model_architecture, 
            path_model_weight = weight_path):
    # Printing the information passed to the Predict Function
    print("Image Path: "+img_path)
    print("Arhitecture Used:")
    print(model_architecture)
    print("Path for Model Weights: ")
    print(path_model_weight)
    # Getting the tensor of image
    image_to_predict = path_to_tensor(img_path).astype('float32')/255
    # Getting the model's architecture
    model = model_architecture
    # Loading the weights
    model.load_weights(path_model_weight)
    # printing the weights
    print("Model Weights: ")
    print(model.load_weights(path_model_weight))
    # Predicting
    pred = model.predict(image_to_predict)
    print("Prediction..." + " Melanoma : ", pred[0][0], " | Other : ", pred[0][1])
    predict_0_0 = pred[0][0]
    predict_0_1 = pred[0][1]
    if np.argmax(pred) == 0:
        return [1., 0.]
    elif np.argmax(pred) == 1:
        return [0., 1.]

"""### 3.1 MobileNet Architecture

#### 3.1.1 Loading the Model and Weights
"""

model_architecture = mobilenet_architecture()
weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5"

"""#### 3.1.3 Predicting for a Sample Image Using MobileNet"""

predict("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Data/Data Image JPG/ISIC_0001769.jpg", model_architecture, weight_path)

"""### 3.2 Inception Architecture

#### 3.2.1 Loading the Model and Weights
"""

model_architecture = inception_architecture()
weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5"

"""#### 3.2.2 Predicting for a Sample Image Using InceptionV3"""

predict("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Data/Data Image JPG/ISIC_0001769.jpg", model_architecture, weight_path)

"""### 3.3 Xception Architecture

#### 3.3.1 Loading the Model and Weights
"""

model_architecture = xception_architecture()
weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5"

"""#### 3.3.2 Predicting a Sample Image using Xception"""

predict("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Validation_Data/Data Image JPG/ISIC_0001769.jpg", model_architecture, weight_path)

"""## 4. Evaluating the Models Individually on Validation Data"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the libraries
from sklearn.metrics import roc_curve, auc
import tqdm
import matplotlib.pyplot as plt
# %matplotlib inline

"""#### Defining Function to calculatae Receiving Operating Characteristic curve"""

def compute_roc(y_true, y_score):
    """ 
    Computing the "Receiving Operating Characteristic curve" and area
    """
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_true, y_score) 
    auroc = auc(false_positive_rate, true_positive_rate) 
    return false_positive_rate, true_positive_rate, auroc

"""#### Defining Function for Plotting the Receiving Operating Characteristic curve"""

def plot_roc(y_true, y_score):
    """ 
    Ploting the Receiving Operating Characteristic curve
    """
    false_positive_rate, true_positive_rate, auroc = compute_roc(y_true, y_score)
    plt.figure(figsize=(10,6))
    plt.grid()
    plt.plot(false_positive_rate, 
             true_positive_rate, 
             color='darkorange',
             lw=2, 
             label='ROC curve (area = {:.2f})'.format(auroc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver operating characteristic example', fontsize=15)
    plt.legend(loc="lower right", fontsize=14)
    plt.show()

plt.style.available

plt.style.use("seaborn-white")

"""### 4.1 MobileNet Architecture

#### 4.1.1 Computing Test Set Predictions
"""

# Compute test set predictions
#model_architecture,path_model_weight
NUMBER_TEST_SAMPLES = 150

mobilenet_architecture_function = mobilenet_architecture()
mobilenet_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5"

y_true_MobileNet = valid_targets[:NUMBER_TEST_SAMPLES]
y_score_MobileNet = []
for index in range(NUMBER_TEST_SAMPLES): #compute one at a time due to memory constraints
    probs = predict(img_path = validation_files[index], model_architecture = mobilenet_architecture_function, path_model_weight = mobilenet_architecture_weight_path)
    print("Real values..." + "Melanoma : ", valid_targets[index][0], " | Other : ", valid_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_MobileNet.append(probs)
    
correct_MobileNet = np.array(y_true_MobileNet) == np.array(y_score_MobileNet)

print("Accuracy = %2.2f%%" % (np.mean(correct_MobileNet)*100))

"""#### 4.1.2 Re-ordering Actual y for ROC"""

# Re-ordering the actual y (for ROC)
y_true_2_MobileNet = []
for i in range(len(y_true_MobileNet)):
    y_true_2_MobileNet.append(y_true_MobileNet[i][0])

"""#### 4.1.3 Re-ordering Predicte y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_MobileNet = []
for i in range(len(y_score_MobileNet)):
    y_score_2_MobileNet.append(y_score_MobileNet[i][0])

"""#### 4.1.4 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_MobileNet, y_score_2_MobileNet)

"""#### 4.1.5 Confusion Matrix

##### 4.1.5.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_MobileNet, FALSE_POSITIVE_MobileNet, TRUE_NEGATIVE_MobileNet, FALSE_NEGATIVE_MobileNet = positive_negative_measurement(y_true_2_MobileNet, y_score_2_MobileNet)
postives_negatives_MobileNet = [[TRUE_POSITIVE_MobileNet, FALSE_POSITIVE_MobileNet], 
                                [FALSE_NEGATIVE_MobileNet, TRUE_NEGATIVE_MobileNet]]

postives_negatives_MobileNet

"""##### 4.1.5.2 Obtaining Labels"""

import seaborn as sns
sns.set()
labels_MobileNet =  np.array([['True positive: ' + str(TRUE_POSITIVE_MobileNet),
                                'False positive: ' + str(FALSE_POSITIVE_MobileNet)],
                                ['False negative: ' + str(FALSE_NEGATIVE_MobileNet),
                                'True negative: ' + str(TRUE_NEGATIVE_MobileNet)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_MobileNet, annot = labels_MobileNet, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

labels_MobileNet

"""##### 4.1.5.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_MobileNet = TRUE_POSITIVE_MobileNet / (TRUE_POSITIVE_MobileNet + FALSE_NEGATIVE_MobileNet)
print("Sensitivity: ", sensitivity_MobileNet)

"""##### 4.1.5.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_MobileNet = TRUE_NEGATIVE_MobileNet / (TRUE_NEGATIVE_MobileNet + FALSE_NEGATIVE_MobileNet)
    print("Specifity: ", specifity_MobileNet)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""##### 4.1.5.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_MobileNet = TRUE_POSITIVE_MobileNet / (TRUE_POSITIVE_MobileNet + FALSE_POSITIVE_MobileNet)
print("Precision: ", predcision_MobileNet)

"""##### 4.1.5.6 Calculating Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_MobileNet = TRUE_NEGATIVE_MobileNet / (TRUE_NEGATIVE_MobileNet + FALSE_NEGATIVE_MobileNet)
    print("Negative predictive value: ", npv_MobileNet)
except:
    print("0 Negative Predictions")

"""##### 4.1.5.7 Calculating Accuracy"""

# Accuracy 
accuracy_MobileNet = (TRUE_POSITIVE_MobileNet + TRUE_NEGATIVE_MobileNet) / (TRUE_POSITIVE_MobileNet + FALSE_POSITIVE_MobileNet + TRUE_NEGATIVE_MobileNet + FALSE_NEGATIVE_MobileNet)
print("Accuracy: ", accuracy_MobileNet)

"""### 4.2 Inception Architecture

#### 4.2.1 Compute Test Set Predictions
"""

# Compute test set predictions
#model_architecture,path_model_weight
NUMBER_TEST_SAMPLES_Inception = 150

inception_architecture_function = inception_architecture()
inception_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5"

y_true_Inception = valid_targets[:NUMBER_TEST_SAMPLES_Inception]
y_score_Inception = []
for index in range(NUMBER_TEST_SAMPLES_Inception): #compute one at a time due to memory constraints
    probs_Inception = predict(img_path = validation_files[index], model_architecture = inception_architecture_function, path_model_weight = inception_architecture_weight_path)
    print("Real values {}...".format(index+1) + "Melanoma : ", valid_targets[index][0], " | Other : ", valid_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Inception.append(probs_Inception)
    
correct_Inception = np.array(y_true_Inception) == np.array(y_score_Inception)

print("Accuracy = %2.2f%%" % (np.mean(correct_Inception)*100))

"""#### 4.2.2 Evaluating the Model

##### 4.2.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Inception = []
for i in range(len(y_true_Inception)):
    y_true_2_Inception.append(y_true_Inception[i][0])

"""##### 4.2.2.2 Re-ordering the Predicte y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Inception = []
for i in range(len(y_score_Inception)):
    y_score_2_Inception.append(y_score_Inception[i][0])

"""##### 4.2.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Inception, y_score_2_Inception)

"""##### 4.2.2.4 Confusion Matrix

###### 4.2.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

"""###### 4.2.2.4.2 Obtaining Labels"""

TRUE_POSITIVE_Inception, FALSE_POSITIVE_Inception, TRUE_NEGATIVE_Inception, FALSE_NEGATIVE_Inception = positive_negative_measurement(y_true_2_Inception, y_score_2_Inception)
postives_negatives_Inception = [[TRUE_POSITIVE_Inception, FALSE_POSITIVE_Inception], 
                                [FALSE_NEGATIVE_Inception, TRUE_NEGATIVE_Inception]]

import seaborn as sns
sns.set()
labels_Inception =  np.array([['True positive: ' + str(TRUE_POSITIVE_Inception),
                     'False positive: ' + str(FALSE_POSITIVE_Inception)],
                    ['False negative: ' + str(FALSE_NEGATIVE_Inception),
                     'True negative: ' + str(TRUE_POSITIVE_Inception)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Inception, annot = labels_Inception, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 4.2.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Inception = TRUE_POSITIVE_Inception / (TRUE_POSITIVE_Inception + FALSE_NEGATIVE_Inception)
print("Sensitivity: ", sensitivity_Inception)

"""###### 4.2.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Inception = TRUE_NEGATIVE_Inception / (TRUE_NEGATIVE_Inception + FALSE_NEGATIVE_Inception)
    print("Specifity: ", specifity_Inception)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 4.2.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Inception = TRUE_POSITIVE_Inception / (TRUE_POSITIVE_Inception + FALSE_POSITIVE_Inception)
print("Precision: ", predcision_Inception)

"""###### 4.2.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Inception = TRUE_NEGATIVE_Inception / (TRUE_NEGATIVE_Inception + FALSE_NEGATIVE_Inception)
    print("Negative predictive value: ", npv_Inception)
except:
    print("0 Negative Predictions")

"""###### 4.2.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Inception = (TRUE_POSITIVE_Inception + TRUE_NEGATIVE_Inception) / (TRUE_POSITIVE_Inception + FALSE_POSITIVE_Inception + TRUE_NEGATIVE_Inception + FALSE_NEGATIVE_Inception)
print("Accuracy: ", accuracy_Inception)

"""### 4.3 Xception Architecture

#### 4.3.1 Compute Test Set Predictions
"""

# Compute test set predictions
#model_architecture,path_model_weight
NUMBER_TEST_SAMPLES_Xception = 150

xception_architecture_function = xception_architecture()
xception_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5"

y_true_Xception = valid_targets[:NUMBER_TEST_SAMPLES_Xception]
y_score_Xception = []
for index in range(NUMBER_TEST_SAMPLES_Xception): #compute one at a time due to memory constraints
    probs_Xception = predict(img_path = validation_files[index], model_architecture = xception_architecture_function, path_model_weight = xception_architecture_weight_path)
    print("Real values {}...".format(index+1) + "Melanoma : ", valid_targets[index][0], " | Other : ", valid_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Xception.append(probs_Xception)
    
correct_Xception = np.array(y_true_Xception) == np.array(y_score_Xception)

print("Accuracy = %2.2f%%" % (np.mean(correct_Xception)*100))

"""#### 4.3.2 Evaluating the Model

##### 4.3.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Xception = []
for i in range(len(y_true_Xception)):
    y_true_2_Xception.append(y_true_Xception[i][0])

"""##### 4.3.2.2 Re-ordering the Predict y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Xception = []
for i in range(len(y_score_Xception)):
    y_score_2_Xception.append(y_score_Xception[i][0])

"""##### 4.3.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Xception, y_score_2_Xception)

"""##### 4.3.2.4 Confusion Matrix

###### 4.3.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_Xception, FALSE_POSITIVE_Xception, TRUE_NEGATIVE_Xception, FALSE_NEGATIVE_Xception = positive_negative_measurement(y_true_2_Xception, y_score_2_Xception)
postives_negatives_Xception = [[TRUE_POSITIVE_Xception, FALSE_POSITIVE_Xception], 
                                [FALSE_NEGATIVE_Xception, TRUE_NEGATIVE_Xception]]

"""###### 4.3.2.4.2 Obtaining the Labels"""

import seaborn as sns
sns.set()
labels_Xception =  np.array([['True positive: ' + str(TRUE_POSITIVE_Xception),
                     'False positive: ' + str(FALSE_POSITIVE_Xception)],
                    ['False negative: ' + str(FALSE_NEGATIVE_Xception),
                     'True negative: ' + str(TRUE_POSITIVE_Xception)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Xception, annot = labels_Xception, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 4.3.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Xception = TRUE_POSITIVE_Xception / (TRUE_POSITIVE_Xception + FALSE_NEGATIVE_Xception)
print("Sensitivity: ", sensitivity_Xception)

"""###### 4.3.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Xception = TRUE_NEGATIVE_Xception / (TRUE_NEGATIVE_Xception + FALSE_NEGATIVE_Xception)
    print("Specifity: ", specifity_Xception)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 4.3.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Xception = TRUE_POSITIVE_Xception / (TRUE_POSITIVE_Xception + FALSE_POSITIVE_Xception)
print("Precision: ", predcision_Xception)

"""###### 4.3.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Xception = TRUE_NEGATIVE_Xception / (TRUE_NEGATIVE_Xception + FALSE_NEGATIVE_Xception)
    print("Negative predictive value: ", npv_Xception)
except:
    print("0 Negative Predictions")

"""###### 4.3.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Xception = (TRUE_POSITIVE_Xception + TRUE_NEGATIVE_Xception) / (TRUE_POSITIVE_Xception + FALSE_POSITIVE_Xception + TRUE_NEGATIVE_Xception + FALSE_NEGATIVE_Xception)
print("Accuracy: ", accuracy_Xception)

"""## 5. Evaluating the Models Together on Validation Data - Ensembling the models"""

from keras.layers import Input

"""### 5.1 Defining the Input Shape"""

# Single input for multiple models
model_input = Input(shape=(512, 512, 3))

"""### 5.2 Defining all the Models"""

def mobilenet_architecture():
    """
    Pre-build architecture of mobilenet for our dataset.
    """
    # Imprting the model
    from keras.applications.mobilenet import MobileNet

    # Pre-build model
    base_model = MobileNet(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    mobilenet_model = Model(base_model.input, output)
    
    # Getting the summary of architecture
    mobilenet_model.summary()
    
    # Compiling the model
    mobilenet_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])

    return mobilenet_model

# Model 1
mobilenet_model = mobilenet_architecture()
mobilenet_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5")

def inception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model 
    from keras.applications.inception_v3 import InceptionV3

    # Pre-build model
    base_model = InceptionV3(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    inception_model = Model(base_model.input, output)
    
    # Summary of the model
    inception_model.summary()
    
    # Compiling the model
    inception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])
    
    return inception_model

# Model 2
inception_model = inception_architecture()
inception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5")

def xception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model
    from keras.applications.xception import Xception

    # Pre-build model
    base_model = Xception(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    xception_model = Model(base_model.input, output)

    # Summary of the model
    xception_model.summary()
    
    # Compiling the model
    xception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                           loss = 'categorical_crossentropy', 
                           metrics = ['accuracy'])

    return xception_model

# Model 3
xception_model = xception_architecture()
xception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5")

"""### 5.3 Appending All the Models"""

# Appending all models
models = [mobilenet_model, inception_model, xception_model]

"""### 5.4 Defining the Ensembling Function"""

def ensemble(models, model_input):
    outputs = [model.outputs[0] for model in models]
    print("Outputs: ")
    print(outputs)
    y = keras.layers.Average()(outputs)
    print("y: ")
    print(y)
    model = Model(model_input, y, name='ensemble')
    print("Model: ")
    print(model)
    return model

# Getting ensemble model
ensemble_model = ensemble(models, model_input)

"""### 5.5 Obtaing the Weights of all the Models combined"""

# average the weights of multiple loaded models
from keras.models import load_model
from keras.models import clone_model
from numpy import average
from numpy import array

# load models from file
def load_all_models(weight_file_names_with_path):
	all_models = list()
	for epoch in range(0,len(weight_file_names_with_path)):
		# define filename for this ensemble
		filename = weight_file_names_with_path[epoch] #'model_' + str(epoch) + '.h5'
		# load model from file
		model = load_model(filename)
		# add to list of members
		all_models.append(model)
		print('>loaded %s' % filename)
	return all_models

weight_file_names_with_path = ["/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5","/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5","/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5"]

# load all models into memory
members = load_all_models(weight_file_names_with_path)
print('Loaded %d models' % len(members))

members

# prepare an array of equal weights
n_models = len(members)
total_weights = [1/n_models for i in range(1, n_models+1)] #[1/3 for i in range(1,(3+1))]

n_models

total_weights

for weight in total_weights:
    weight = float(weight)

total_weights

type(total_weights)

type(total_weights[0])

new_model = None

weights_of_MobileNet = members[0].get_weights()
print(weights_of_MobileNet)

weights_of_Inception = members[1].get_weights()
print(weights_of_Inception)

weights_of_Xception = members[2].get_weights()
print(weights_of_Xception)

# # np.concatenate(weights_of_MobileNet,weights_of_Inception)
# # using naive method to concat
# for i in weights_of_Inception :
#     weights_of_MobileNet.append(i)
# # weights_of_MobileNet_and_Inception = weights_of_MobileNet.append(weights_of_Inception)
# print("weights_of_MobileNet_and_Inception: ")
# print(weights_of_MobileNet)

# # np.concatenate(weights_of_MobileNet,weights_of_Inception)
# # using naive method to concat
# for i in weights_of_Xception :
#     weights_of_MobileNet.append(i)
# # weights_of_MobileNet_and_Inception = weights_of_MobileNet.append(weights_of_Inception)
# print("weights_of_MobileNet_and_Inception: ")
# print(weights_of_MobileNet)

print(type(weights_of_MobileNet))

weights_of_MobileNet_and_Inception = np.concatenate((weights_of_MobileNet[0], weights_of_Inception[0]))
print(type(weights_of_MobileNet_and_Inception))

print(weights_of_MobileNet_and_Inception.shape)

weights_of_MobileNet_Inception_and_Xception = c = np.concatenate((weights_of_MobileNet_and_Inception, weights_of_Xception[0])) #np.stack((weights_of_MobileNet_and_Inception,b), axis=3) 
print(type(weights_of_MobileNet_Inception_and_Xception))
print(weights_of_MobileNet_Inception_and_Xception.shape)

from keras.models import Sequential
from keras.layers import Dense
from keras.models import model_from_json

# serialize model to JSON
model_weights_for_json = ensemble_model.to_json()
with open("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/Ensemble Model/ensemble_model.json", "w") as json_file:
    json_file.write(model_weights_for_json)
# serialize weights to HDF5
ensemble_model.save_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/Ensemble Model/ensemble_model.h5")
print("Saved model to disk")

# #create a model from the weights of multiple models
# def model_weight_ensemble(members, weights):
#     #determine how many layers need to be averaged
#     n_layers = len(members[0].get_weights())
#     print("No. of Layers which need to be averaged: ")
#     print(n_layers)
#     print("The weights of the Member 0 are: ")
#     print(members[0].get_weights())
#     # avg_model_weights = list()
#     # for weight in weights:
#     #     weight = float(weight)
#     # for layer in range(n_layers):
#     #     #collect this layer from each model
#     #     layer_weights = array([model.get_weights()[layer] for model in members])
#     #     # weighted average of weights for this layer
#     #     avg_layer_weights = average(layer_weights, axis = 0, weights=weights)
#     #     #store average layer weights
#     #     avg_model_weights.append(avg_layer_weights)
#     # # create a new model with the same structure
#     # model = clone_model(members[0])
#     # # set the weights in the new
#     # model.set_weights(avg_model_weights)
#     # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#     return model

# create a new model with the weighted average of all model weights
# new_model = model_weight_ensemble(members, weights)

# new_model

# # summarize the created model
# new_model.summary()

"""### 5.5 Evaluating ensemble model"""

# Compute test set predictions
NUMBER_TEST_SAMPLES_Ensemble = 150

y_true_Ensemble = valid_targets[:NUMBER_TEST_SAMPLES_Ensemble]
y_score_Ensemble = []
for index in range(NUMBER_TEST_SAMPLES_Ensemble): #compute one at a time due to memory constraints
    image_to_predict_Ensemble = path_to_tensor(validation_files[index]).astype("float32")/255.
    probs_Ensemble = ensemble_model.predict(image_to_predict_Ensemble,)
    if np.argmax(probs_Ensemble) == 0:
        y_score_Ensemble.append([1., 0.])
    elif np.argmax(probs_Ensemble) == 1:
        y_score_Ensemble.append([0., 1.])
    print("Predicted value {}... ".format(index+1) + " Melanoma : ", probs_Ensemble[0][0],  " | Other : ", probs_Ensemble[0][1])
    print("Real values {}...".format(index+1) + "      Melanoma : ", valid_targets[index][0], "      | Other : ", valid_targets[index][1])
    print("---------------------------------------------------------------------------")
    
    
correct_Ensemble = np.array(y_true_Ensemble) == np.array(y_score_Ensemble)

print("Accuracy = %2.2f%%" % (np.mean(correct_Ensemble)*100))

image_to_predict_Ensemble = path_to_tensor("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Output_melanoma/ISIC_0000000_180_angle_flipped.jpg").astype('float32')/255.
ensemble_model.predict(image_to_predict_Ensemble)

"""#### 5.5.1 Compute Test Set Predictions"""

def predict_ensemble(img_path,
            model_architecture = model_architecture, 
            path_model_weight = weight_path):
    # Printing the information passed to the Predict Function
    print("Image Path: ")
    print(img_path)
    print("Arhitecture Used:")
    print(model_architecture)
    print("Path for Model Weights: ")
    print(path_model_weight)
    # Getting the tensor of image
    image_to_predict = path_to_tensor(img_path).astype('float32')/255
    # Getting the model's architecture
    model = model_architecture
    # Loading the weights
    model.load_weights(path_model_weight)
    # printing the weights
    print("Model Weights: ")
    print(model.load_weights(path_model_weight))
    # Predicting
    pred = model.predict(image_to_predict)
    print("Prediction..." + " Melanoma : ", pred[0][0], " | Other : ", pred[0][1])
    predict_0_0 = pred[0][0]
    predict_0_1 = pred[0][1]
    if np.argmax(pred) == 0:
        return [1., 0.]
    elif np.argmax(pred) == 1:
        return [0., 1.]

# Compute test set predictions
#model_architecture,path_model_weight
NUMBER_TEST_SAMPLES_Ensemble = 150

all_weights_combined_as_list = weights_of_MobileNet_Inception_and_Xception

y_true_Ensemble = valid_targets[:NUMBER_TEST_SAMPLES_Ensemble]
y_score_Ensemble = []
for index in range(NUMBER_TEST_SAMPLES_Ensemble): #compute one at a time due to memory constraints
    probs_Ensemble = predict_ensemble(img_path = validation_files[index], model_architecture = ensemble_model, path_model_weight = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/Ensemble Model/ensemble_model.h5")
    print("Real values {}...".format(index+1) + "Melanoma : ", valid_targets[index][0], " | Other : ", valid_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Ensemble.append(probs_Ensemble)
    
correct_Ensemble = np.array(y_true_Ensemble) == np.array(y_score_Ensemble)

print("Accuracy = %2.2f%%" % (np.mean(correct_Ensemble)*100))

"""#### 5.5.2 Evaluating the Model

##### 5.5.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Ensemble = []
for i in range(len(y_true_Ensemble)):
    y_true_2_Ensemble.append(y_true_Ensemble[i][0])

"""##### 5.5.2.2 Re-ordering the Predict y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Ensemble = []
for i in range(len(y_score_Ensemble)):
    y_score_2_Ensemble.append(y_score_Ensemble[i][0])

"""##### 5.5.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Ensemble, y_score_2_Ensemble)

"""##### 5.5.2.4 Confusion Matrix

###### 5.5.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_Ensemble, FALSE_POSITIVE_Ensemble, TRUE_NEGATIVE_Ensemble, FALSE_NEGATIVE_Ensemble = positive_negative_measurement(y_true_2_Ensemble, y_score_2_Ensemble)
postives_negatives_Ensemble = [[TRUE_POSITIVE_Ensemble, FALSE_POSITIVE_Ensemble], 
                                [FALSE_NEGATIVE_Ensemble, TRUE_NEGATIVE_Ensemble]]

"""###### 5.5.2.4.2 Obtaining the Labels"""

import seaborn as sns
sns.set()
labels_Ensemble =  np.array([['True positive: ' + str(TRUE_POSITIVE_Ensemble),
                     'False positive: ' + str(FALSE_POSITIVE_Ensemble)],
                    ['False negative: ' + str(FALSE_NEGATIVE_Ensemble),
                     'True negative: ' + str(TRUE_POSITIVE_Ensemble)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Ensemble, annot = labels_Ensemble, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 5.5.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Ensemble = TRUE_POSITIVE_Ensemble / (TRUE_POSITIVE_Ensemble + FALSE_NEGATIVE_Ensemble)
print("Sensitivity: ", sensitivity_Ensemble)

"""###### 5.5.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Ensemble = TRUE_NEGATIVE_Ensemble / (TRUE_NEGATIVE_Ensemble + FALSE_NEGATIVE_Ensemble)
    print("Specifity: ", specifity_Ensemble)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 5.5.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Ensemble = TRUE_POSITIVE_Ensemble / (TRUE_POSITIVE_Ensemble + FALSE_POSITIVE_Ensemble)
print("Precision: ", predcision_Ensemble)

"""###### 5.5.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Ensemble = TRUE_NEGATIVE_Ensemble / (TRUE_NEGATIVE_Ensemble + FALSE_NEGATIVE_Ensemble)
    print("Negative predictive value: ", npv_Ensemble)
except:
    print("0 Negative Predictions")

"""###### 5.5.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Ensemble = (TRUE_POSITIVE_Ensemble + TRUE_NEGATIVE_Ensemble) / (TRUE_POSITIVE_Ensemble + FALSE_POSITIVE_Ensemble + TRUE_NEGATIVE_Ensemble + FALSE_NEGATIVE_Ensemble)
print("Accuracy: ", accuracy_Ensemble)

"""## 6. Evaluating the Models Individually on Testing Data"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the libraries
from sklearn.metrics import roc_curve, auc
import tqdm
import matplotlib.pyplot as plt
# %matplotlib inline

"""#### Defining Function to calculatae Receiving Operating Characteristic curve"""

def compute_roc(y_true, y_score):
    """ 
    Computing the "Receiving Operating Characteristic curve" and area
    """
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_true, y_score) 
    auroc = auc(false_positive_rate, true_positive_rate) 
    return false_positive_rate, true_positive_rate, auroc

"""#### Defining Function for Plotting the Receiving Operating Characteristic curve"""

def plot_roc(y_true, y_score):
    """ 
    Ploting the Receiving Operating Characteristic curve
    """
    false_positive_rate, true_positive_rate, auroc = compute_roc(y_true, y_score)
    plt.figure(figsize=(10,6))
    plt.grid()
    plt.plot(false_positive_rate, 
             true_positive_rate, 
             color='darkorange',
             lw=2, 
             label='ROC curve (area = {:.2f})'.format(auroc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver operating characteristic example', fontsize=15)
    plt.legend(loc="lower right", fontsize=14)
    plt.show()

plt.style.available

plt.style.use("seaborn-white")

"""### 6.1 MobileNet Architecture

#### 6.1.1 Computing Test Set Predictions
"""

print("No. of Files in Test Data")
print(len(test_files))
print("No. of Target Values in Test Data")
print(len(test_targets))

# Compute test set predictions
NUMBER_TEST_SAMPLES_MobileNet_Test = 600

mobilenet_architecture_function = mobilenet_architecture()
mobilenet_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5"

y_true_MobileNet_Test = test_targets[:NUMBER_TEST_SAMPLES_MobileNet_Test]
print(y_true_MobileNet_Test)

y_score_MobileNet_Test = []
count = 0
for index in range(NUMBER_TEST_SAMPLES_MobileNet_Test): #compute one at a time due to memory constraints
    count = count+1
    print(count)
    probs_MobileNet_Test = predict(img_path = test_files[index], model_architecture = mobilenet_architecture_function, path_model_weight = mobilenet_architecture_weight_path)
    print("Real values..." + "Melanoma : ", test_targets[index][0], " | Other : ", test_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_MobileNet_Test.append(probs_MobileNet_Test)
    
correct_MobileNet_Test = np.array(y_true_MobileNet_Test) == np.array(y_score_MobileNet_Test)

print("Accuracy = %2.2f%%" % (np.mean(correct_MobileNet_Test)*100))

"""#### 6.1.2 Re-ordering Actual y for ROC"""

# Re-ordering the actual y (for ROC)
y_true_2_MobileNet_Test = []
for i in range(len(y_true_MobileNet_Test)):
    y_true_2_MobileNet_Test.append(y_true_MobileNet_Test[i][0])

"""#### 6.1.3 Re-ordering Predicte y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_MobileNet_Test = []
for i in range(len(y_score_MobileNet_Test)):
    y_score_2_MobileNet_Test.append(y_score_MobileNet_Test[i][0])

"""#### 6.1.4 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_MobileNet_Test, y_score_2_MobileNet_Test)

"""#### 6.1.5 Confusion Matrix

##### 6.1.5.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_MobileNet_Test, FALSE_POSITIVE_MobileNet_Test, TRUE_NEGATIVE_MobileNet_Test, FALSE_NEGATIVE_MobileNet_Test = positive_negative_measurement(y_true_2_MobileNet_Test, y_score_2_MobileNet_Test)
postives_negatives_MobileNet_Test = [[TRUE_POSITIVE_MobileNet_Test, FALSE_POSITIVE_MobileNet_Test], 
                                     [FALSE_NEGATIVE_MobileNet_Test, TRUE_NEGATIVE_MobileNet_Test]]

postives_negatives_MobileNet_Test

"""##### 6.1.5.2 Obtaining Labels"""

import seaborn as sns
sns.set()
labels_MobileNet_Test =  np.array([['True positive: ' + str(TRUE_POSITIVE_MobileNet_Test),
                                    'False positive: ' + str(FALSE_POSITIVE_MobileNet_Test)],
                                    ['False negative: ' + str(FALSE_NEGATIVE_MobileNet_Test),
                                    'True negative: ' + str(TRUE_NEGATIVE_MobileNet_Test)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_MobileNet_Test, annot = labels_MobileNet_Test, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

labels_MobileNet_Test

"""##### 6.1.5.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_MobileNet_Test = TRUE_POSITIVE_MobileNet_Test / (TRUE_POSITIVE_MobileNet_Test + FALSE_NEGATIVE_MobileNet_Test)
print("Sensitivity: ", sensitivity_MobileNet_Test)

"""##### 6.1.5.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_MobileNet_Test = TRUE_NEGATIVE_MobileNet_Test / (TRUE_NEGATIVE_MobileNet_Test + FALSE_NEGATIVE_MobileNet_Test)
    print("Specifity: ", specifity_MobileNet_Test)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""##### 6.1.5.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_MobileNet_Test = TRUE_POSITIVE_MobileNet_Test / (TRUE_POSITIVE_MobileNet_Test + FALSE_POSITIVE_MobileNet_Test)
print("Precision: ", predcision_MobileNet_Test)

"""##### 6.1.5.6 Calculating Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_MobileNet_Test = TRUE_NEGATIVE_MobileNet_Test / (TRUE_NEGATIVE_MobileNet_Test + FALSE_NEGATIVE_MobileNet_Test)
    print("Negative predictive value: ", npv_MobileNet_Test)
except:
    print("0 Negative Predictions")

"""##### 6.1.5.7 Calculating Accuracy"""

# Accuracy 
accuracy_MobileNet_Test = (TRUE_POSITIVE_MobileNet_Test + TRUE_NEGATIVE_MobileNet_Test) / (TRUE_POSITIVE_MobileNet_Test + FALSE_POSITIVE_MobileNet_Test + TRUE_NEGATIVE_MobileNet_Test + FALSE_NEGATIVE_MobileNet_Test)
print("Accuracy: ", accuracy_MobileNet_Test)

"""### 6.2 Inception Architecture

#### 6.2.1 Compute Test Set Predictions
"""

# Compute test set predictions
NUMBER_TEST_SAMPLES_Inception_Test = 600

inception_architecture_function = inception_architecture()
inception_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5"

y_true_Inception_Test = test_targets[:NUMBER_TEST_SAMPLES_Inception_Test]
y_score_Inception_Test = []
for index in range(NUMBER_TEST_SAMPLES_Inception_Test): #compute one at a time due to memory constraints
    probs_Inception_Test = predict(img_path = test_files[index], model_architecture = inception_architecture_function, path_model_weight = inception_architecture_weight_path)
    print("Real values {}...".format(index+1) + "Melanoma : ", test_targets[index][0], " | Other : ", test_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Inception_Test.append(probs_Inception_Test)
    
correct_Inception_Test = np.array(y_true_Inception_Test) == np.array(y_score_Inception_Test)

print("Accuracy = %2.2f%%" % (np.mean(correct_Inception_Test)*100))

"""#### 6.2.2 Evaluating the Model

##### 6.2.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Inception_Test = []
for i in range(len(y_true_Inception_Test)):
    y_true_2_Inception_Test.append(y_true_Inception_Test[i][0])

"""##### 6.2.2.2 Re-ordering the Predicte y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Inception_Test = []
for i in range(len(y_score_Inception_Test)):
    y_score_2_Inception_Test.append(y_score_Inception_Test[i][0])

"""##### 6.2.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Inception_Test, y_score_2_Inception_Test)

"""##### 6.2.2.4 Confusion Matrix

###### 6.2.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

"""###### 6.2.2.4.2 Obtaining Labels"""

TRUE_POSITIVE_Inception_Test, FALSE_POSITIVE_Inception_Test, TRUE_NEGATIVE_Inception_Test, FALSE_NEGATIVE_Inception_Test = positive_negative_measurement(y_true_2_Inception_Test, y_score_2_Inception_Test)
postives_negatives_Inception_Test = [[TRUE_POSITIVE_Inception_Test, FALSE_POSITIVE_Inception_Test], 
                                     [FALSE_NEGATIVE_Inception_Test, TRUE_NEGATIVE_Inception_Test]]

import seaborn as sns
sns.set()
labels_Inception_Test =  np.array([['True positive: ' + str(TRUE_POSITIVE_Inception_Test),
                                    'False positive: ' + str(FALSE_POSITIVE_Inception_Test)],
                                    ['False negative: ' + str(FALSE_NEGATIVE_Inception_Test),
                                    'True negative: ' + str(TRUE_POSITIVE_Inception_Test)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Inception_Test, annot = labels_Inception_Test, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 6.2.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Inception_Test = TRUE_POSITIVE_Inception_Test / (TRUE_POSITIVE_Inception_Test + FALSE_NEGATIVE_Inception_Test)
print("Sensitivity: ", sensitivity_Inception_Test)

"""###### 6.2.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Inception_Test = TRUE_NEGATIVE_Inception_Test / (TRUE_NEGATIVE_Inception_Test + FALSE_NEGATIVE_Inception_Test)
    print("Specifity: ", specifity_Inception_Test)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 6.2.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Inception_Test = TRUE_POSITIVE_Inception_Test / (TRUE_POSITIVE_Inception_Test + FALSE_POSITIVE_Inception_Test)
print("Precision: ", predcision_Inception_Test)

"""###### 6.2.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Inception_Test = TRUE_NEGATIVE_Inception_Test / (TRUE_NEGATIVE_Inception_Test + FALSE_NEGATIVE_Inception_Test)
    print("Negative predictive value: ", npv_Inception_Test)
except:
    print("0 Negative Predictions")

"""###### 6.2.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Inception_Test = (TRUE_POSITIVE_Inception_Test + TRUE_NEGATIVE_Inception_Test) / (TRUE_POSITIVE_Inception_Test + FALSE_POSITIVE_Inception_Test + TRUE_NEGATIVE_Inception_Test + FALSE_NEGATIVE_Inception_Test)
print("Accuracy: ", accuracy_Inception_Test)

"""### 6.3 Xception Architecture

#### 6.3.1 Compute Test Set Predictions
"""

# Compute test set predictions
NUMBER_TEST_SAMPLES_Xception_Test = 600

xception_architecture_function = xception_architecture()
xception_architecture_weight_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5"


y_true_Xception_Test = test_targets[:NUMBER_TEST_SAMPLES_Xception_Test]
y_score_Xception_Test = []
for index in range(NUMBER_TEST_SAMPLES_Xception_Test): #compute one at a time due to memory constraints
    probs_Xception_Test = predict(img_path = test_files[index], model_architecture = xception_architecture_function, path_model_weight = xception_architecture_weight_path)
    print("Real values {}...".format(index+1) + "Melanoma : ", test_targets[index][0], " | Other : ", test_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Xception_Test.append(probs_Xception_Test)
    
correct_Xception_Test = np.array(y_true_Xception_Test) == np.array(y_score_Xception_Test)

print("Accuracy = %2.2f%%" % (np.mean(correct_Xception_Test)*100))

"""#### 6.3.2 Evaluating the Model

##### 6.3.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Xception_Test = []
for i in range(len(y_true_Xception_Test)):
    y_true_2_Xception_Test.append(y_true_Xception_Test[i][0])

"""##### 6.3.2.2 Re-ordering the Predict y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Xception_Test = []
for i in range(len(y_score_Xception_Test)):
    y_score_2_Xception_Test.append(y_score_Xception_Test[i][0])

"""##### 6.3.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Xception_Test, y_score_2_Xception_Test)

"""##### 6.3.2.4 Confusion Matrix

###### 6.3.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_Xception_Test, FALSE_POSITIVE_Xception_Test, TRUE_NEGATIVE_Xception_Test, FALSE_NEGATIVE_Xception_Test = positive_negative_measurement(y_true_2_Xception_Test, y_score_2_Xception_Test)
postives_negatives_Xception_Test = [[TRUE_POSITIVE_Xception_Test, FALSE_POSITIVE_Xception_Test], 
                                    [FALSE_NEGATIVE_Xception_Test, TRUE_NEGATIVE_Xception_Test]]

"""###### 6.3.2.4.2 Obtaining the Labels"""

import seaborn as sns
sns.set()
labels_Xception_Test =  np.array([['True positive: ' + str(TRUE_POSITIVE_Xception_Test),
                                    'False positive: ' + str(FALSE_POSITIVE_Xception_Test)],
                                    ['False negative: ' + str(FALSE_NEGATIVE_Xception_Test),
                                    'True negative: ' + str(TRUE_POSITIVE_Xception_Test)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Xception_Test, annot = labels_Xception_Test, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 6.3.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Xception_Test = TRUE_POSITIVE_Xception_Test / (TRUE_POSITIVE_Xception_Test + FALSE_NEGATIVE_Xception_Test)
print("Sensitivity: ", sensitivity_Xception_Test)

"""###### 6.3.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Xception_Test = TRUE_NEGATIVE_Xception_Test / (TRUE_NEGATIVE_Xception_Test + FALSE_NEGATIVE_Xception_Test)
    print("Specifity: ", specifity_Xception_Test)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 6.3.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Xception_Test = TRUE_POSITIVE_Xception_Test / (TRUE_POSITIVE_Xception_Test + FALSE_POSITIVE_Xception_Test)
print("Precision: ", predcision_Xception_Test)

"""###### 6.3.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Xception_Test = TRUE_NEGATIVE_Xception_Test / (TRUE_NEGATIVE_Xception_Test + FALSE_NEGATIVE_Xception_Test)
    print("Negative predictive value: ", npv_Xception_Test)
except:
    print("0 Negative Predictions")

"""###### 6.3.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Xception_Test = (TRUE_POSITIVE_Xception_Test + TRUE_NEGATIVE_Xception_Test) / (TRUE_POSITIVE_Xception_Test + FALSE_POSITIVE_Xception_Test + TRUE_NEGATIVE_Xception_Test + FALSE_NEGATIVE_Xception_Test)
print("Accuracy: ", accuracy_Xception_Test)

"""## 7. Evaluating the Models Together on Testing Data - Ensembling the models"""

from keras.layers import Input

"""### 7.1 Defining the Input Shape"""

# Single input for multiple models
model_input = Input(shape=(512, 512, 3))

"""### 7.2 Defining all the Models"""

def mobilenet_architecture():
    """
    Pre-build architecture of mobilenet for our dataset.
    """
    # Imprting the model
    from keras.applications.mobilenet import MobileNet

    # Pre-build model
    base_model = MobileNet(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    mobilenet_model = Model(base_model.input, output)
    
    # Getting the summary of architecture
    mobilenet_model.summary()
    
    # Compiling the model
    mobilenet_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])

    return mobilenet_model

# Model 1
mobilenet_model = mobilenet_architecture()
mobilenet_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5")

def inception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model 
    from keras.applications.inception_v3 import InceptionV3

    # Pre-build model
    base_model = InceptionV3(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    inception_model = Model(base_model.input, output)
    
    # Summary of the model
    inception_model.summary()
    
    # Compiling the model
    inception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                            loss = 'categorical_crossentropy', 
                            metrics = ['accuracy'])
    
    return inception_model

# Model 2
inception_model = inception_architecture()
inception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.InceptionV3.hdf5")

def xception_architecture():
    """
    Pre-build architecture of inception for our dataset.
    """
    # Imprting the model
    from keras.applications.xception import Xception

    # Pre-build model
    base_model = Xception(include_top = False, weights = None, input_tensor = model_input)

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    xception_model = Model(base_model.input, output)

    # Summary of the model
    xception_model.summary()
    
    # Compiling the model
    xception_model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                           loss = 'categorical_crossentropy', 
                           metrics = ['accuracy'])

    return xception_model

# Model 3
xception_model = xception_architecture()
xception_model.load_weights("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/xception_weights.hdf5")

"""### 7.3 Appending All the Models"""

# Appending all models
models = [mobilenet_model, inception_model, xception_model]

"""### 7.4 Defining the Ensembling Function"""

def ensemble(models, model_input):
    outputs = [model.outputs[0] for model in models]
    y = keras.layers.Average()(outputs)
    model = Model(model_input, y, name='ensemble')
    return model

# Getting ensemble model
ensemble_model = ensemble(models, model_input)

"""### 7.5 Evaluating ensemble model"""

# Compute test set predictions
#model_architecture,path_model_weight
NUMBER_TEST_SAMPLES_Ensemble_Test = 600

all_weights_combined_as_list = weights_of_MobileNet_Inception_and_Xception

y_true_Ensemble_Test = test_targets[:NUMBER_TEST_SAMPLES_Ensemble_Test]
y_score_Ensemble_Test = []
for index in range(NUMBER_TEST_SAMPLES_Ensemble_Test): #compute one at a time due to memory constraints
    probs_Ensemble_Test = predict_ensemble(img_path = test_files[index], model_architecture = ensemble_model, path_model_weight = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/Ensemble Model/ensemble_model.h5")
    print("Real values {}...".format(index+1) + "Melanoma : ", test_targets[index][0], " | Other : ", test_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Ensemble_Test.append(probs_Ensemble_Test)
    
correct_Ensemble_Test = np.array(y_true_Ensemble_Test) == np.array(y_score_Ensemble_Test)

print("Accuracy = %2.2f%%" % (np.mean(correct_Ensemble_Test)*100))

image_to_predict_Ensemble_Test = path_to_tensor("/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Output_melanoma/ISIC_0000000_180_angle_flipped.jpg").astype('float32')/255.
ensemble_model.predict(image_to_predict_Ensemble_Test)

"""#### 7.5.1 Compute Test Set Predictions"""

# Compute test set predictions
NUMBER_TEST_SAMPLES_Ensemble_Test = 600

all_weights_combined_as_list = weights_of_MobileNet_Inception_and_Xception

y_true_Ensemble_Test = test_targets[:NUMBER_TEST_SAMPLES_Ensemble_Test]
y_score_Ensemble_Test = []
for index in range(NUMBER_TEST_SAMPLES_Ensemble_Test): #compute one at a time due to memory constraints
    probs_Ensemble_Test = predict(img_path = test_files[index], model_architecture = ensemble_model, path_model_weight = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/Ensemble Model/ensemble_model.h5")
    print("Real values {}...".format(index+1) + "Melanoma : ", test_targets[index][0], " | Other : ", test_targets[index][1])
    print("---------------------------------------------------------------------------")
    y_score_Ensemble_Test.append(probs_Ensemble_Test)
    
correct_Ensemble_Test = np.array(y_true_Ensemble_Test) == np.array(y_score_Ensemble_Test)

print("Accuracy = %2.2f%%" % (np.mean(correct_Ensemble_Test)*100))

"""#### 7.5.2 Evaluating the Model

##### 7.5.2.1 Re-ordering the Actual y for ROC
"""

# Re-ordering the actual y (for ROC)
y_true_2_Ensemble_Test = []
for i in range(len(y_true_Ensemble_Test)):
    y_true_2_Ensemble_Test.append(y_true_Ensemble_Test[i][0])

"""##### 7.5.2.2 Re-ordering the Predict y for ROC"""

# Re-ordering the predicte y (for ROC)
y_score_2_Ensemble_Test = []
for i in range(len(y_score_Ensemble_Test)):
    y_score_2_Ensemble_Test.append(y_score_Ensemble_Test[i][0])

"""##### 7.5.2.3 Plotting the Re-ordered ROC"""

plot_roc(y_true_2_Ensemble_Test, y_score_2_Ensemble_Test)

"""##### 7.5.2.4 Confusion Matrix

###### 7.5.2.4.1 Defining the Confusion Matrix Function
"""

def positive_negative_measurement(y_true, y_score):
    # Initialization
    TRUE_POSITIVE = 0
    FALSE_POSITIVE = 0
    TRUE_NEGATIVE = 0
    FALSE_NEGATIVE = 0
    
    # Calculating the model
    for i in range(len(y_score)):
        if y_true[i] == y_score[i] == 1:
            TRUE_POSITIVE += 1
        if (y_score[i] == 1) and (y_true[i] != y_score[i]):
            FALSE_POSITIVE += 1
        if y_true[i] == y_score[i] == 0:
            TRUE_NEGATIVE += 1
        if (y_score[i] == 0) and (y_true[i] != y_score[i]):
            FALSE_NEGATIVE += 1

    return(TRUE_POSITIVE, FALSE_POSITIVE, TRUE_NEGATIVE, FALSE_NEGATIVE)

TRUE_POSITIVE_Ensemble_Test, FALSE_POSITIVE_Ensemble_Test, TRUE_NEGATIVE_Ensemble_Test, FALSE_NEGATIVE_Ensemble_Test = positive_negative_measurement(y_true_2_Ensemble_Test, y_score_2_Ensemble_Test)
postives_negatives_Ensemble_Test = [[TRUE_POSITIVE_Ensemble_Test, FALSE_POSITIVE_Ensemble_Test], 
                                    [FALSE_NEGATIVE_Ensemble_Test, TRUE_NEGATIVE_Ensemble_Test]]

"""###### 7.5.2.4.2 Obtaining the Labels"""

import seaborn as sns
sns.set()
labels_Ensemble_Test =  np.array([['True positive: ' + str(TRUE_POSITIVE_Ensemble_Test),
                                    'False positive: ' + str(FALSE_POSITIVE_Ensemble_Test)],
                                    ['False negative: ' + str(FALSE_NEGATIVE_Ensemble_Test),
                                    'True negative: ' + str(TRUE_POSITIVE_Ensemble_Test)]])
plt.figure(figsize = (13, 10))
sns.heatmap(postives_negatives_Ensemble_Test, annot = labels_Ensemble_Test, linewidths = 0.1, fmt="", cmap = 'RdYlGn')

"""###### 7.5.2.4.3 Calculating Sensitivity/Recall/Hit Rate/True Positive Rate"""

# Sensitivity | Recall | hit rate | true positive rate (TPR)
sensitivity_Ensemble_Test = TRUE_POSITIVE_Ensemble_Test / (TRUE_POSITIVE_Ensemble_Test + FALSE_NEGATIVE_Ensemble_Test)
print("Sensitivity: ", sensitivity_Ensemble_Test)

"""###### 7.5.2.4.4 Calculating Specificity/Selectivity/True Negative Rate"""

# Specificity | selectivity | true negative rate (TNR)
try:
    specifity_Ensemble_Test = TRUE_NEGATIVE_Ensemble_Test / (TRUE_NEGATIVE_Ensemble_Test + FALSE_NEGATIVE_Ensemble_Test)
    print("Specifity: ", specifity_Ensemble_Test)
except:
    print("No Specificity due to NO NEGATIVE results.")

"""###### 7.5.2.4.5 Calculating Precision/Positive Predictive Value"""

# Precision | positive predictive value (PPV)
predcision_Ensemble_Test = TRUE_POSITIVE_Ensemble_Test / (TRUE_POSITIVE_Ensemble_Test + FALSE_POSITIVE_Ensemble_Test)
print("Precision: ", predcision_Ensemble_Test)

"""###### 7.5.2.4.6 Negative Predictive Value"""

# Negative predictive value (NPV)
try:
    npv_Ensemble_Test = TRUE_NEGATIVE_Ensemble_Test / (TRUE_NEGATIVE_Ensemble_Test + FALSE_NEGATIVE_Ensemble_Test)
    print("Negative predictive value: ", npv_Ensemble_Test)
except:
    print("0 Negative Predictions")

"""###### 7.5.2.4.7 Calculating Accuracy"""

# Accuracy 
accuracy_Ensemble_Test = (TRUE_POSITIVE_Ensemble_Test + TRUE_NEGATIVE_Ensemble_Test) / (TRUE_POSITIVE_Ensemble_Test + FALSE_POSITIVE_Ensemble_Test + TRUE_NEGATIVE_Ensemble_Test + FALSE_NEGATIVE_Ensemble_Test)
print("Accuracy: ", accuracy_Ensemble_Test)

"""## 8. Localization"""

# Importing the libraries
from keras.applications.mobilenet import preprocess_input
import scipy
import cv2

"""### 8.1 Obtaining the Path for the MobileNet Architecture Models"""

path_to_model_weight = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/Saved Models/weights.best.mobilenet.hdf5"

"""### 8.2 Sample Image Path"""

img_path = "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/ISIC-2017_Test_v2_Data/Data Image JPG/ISIC_0012086.jpg"

"""### 8.3 Defining the function for the Layer Weights for MobileNet"""

def getting_two_layer_weights(path_model_weight = path_to_model_weight):
    # The model

    # Imprting the model
    from keras.applications.mobilenet import MobileNet

    # Pre-build model
    base_model = MobileNet(include_top = False, weights = None, input_shape = (512, 512, 3))

    # Adding output layers
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Dense(units = 2, activation = 'softmax')(x)

    # Creating the whole model
    model = Model(base_model.input, output)
    #model.summary()

    # Compiling the model
    model.compile(optimizer = keras.optimizers.Adam(lr = 0.001), 
                  loss = 'categorical_crossentropy', 
                  metrics = ['accuracy'])
    
    # loading the weights
    model.load_weights(path_model_weight)
    
    # Getting the AMP layer weight
    all_amp_layer_weights = model.layers[-1].get_weights()[0]
    
    # Extracting the wanted output
    mobilenet_model = Model(inputs = model.input, outputs = (model.layers[-3].output, model.layers[-1].output))
    
    return mobilenet_model, all_amp_layer_weights

mobilenet_model, all_amp_layer_weights = getting_two_layer_weights(path_to_model_weight)

"""### 8.4 Defining Class Activation Map Function"""

def mobilenet_CAM(img_path, model, all_amp_layer_weights):
    # Getting filtered images from last convolutional layer + model prediction output
    last_conv_output, predictions = model.predict(path_to_tensor(img_path)) # last_conv_output.shape = (1, 16, 16, 1024)
    
    # Converting the dimension of last convolutional layer to 16 x 16 x 1024     
    last_conv_output = np.squeeze(last_conv_output)
    
    # Model's prediction
    predicted_class = np.argmax(predictions)
    
    # Bilinear upsampling (resize each image to size of original image)
    mat_for_mult = scipy.ndimage.zoom(last_conv_output, (32, 32, 1), order = 1)  # dim from (16, 16, 1024) to (512, 512, 1024)
    
    # Getting the AMP layer weights
    amp_layer_weights = all_amp_layer_weights[:, predicted_class] # dim: (1024,)    
    
    # CAM for object class that is predicted to be in the image
    final_output = np.dot(mat_for_mult, amp_layer_weights) # dim: 512 x 512

    # Return class activation map (CAM)
    return final_output, predicted_class

final_output, predicted_class = mobilenet_CAM(img_path, mobilenet_model, all_amp_layer_weights)

"""### 8.5 Plotting the Class Activation Function for MobileNet"""

def plot_CAM(img_path, ax, model, all_amp_layer_weights):
    # Loading the image / resizing to 512x512 / Converting BGR to RGB
    #im = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB), (512, 512))
    im = path_to_tensor(img_path).astype("float32")/255.
    
    # Plotting the image
    ax.imshow(im.squeeze(), vmin=0, vmax=255)
    
    # Getting the class activation map
    CAM, pred = mobilenet_CAM(img_path, model, all_amp_layer_weights)
    
    CAM = (CAM - CAM.min()) / (CAM.max() - CAM.min())
    
    # Plotting the class activation map
    ax.imshow(CAM, cmap = "jet", alpha = 0.5, interpolation='nearest', vmin=0, vmax=1)

"""### 8.6 Visualizing the Images"""

# Visualizing images with and without localization
# Canvas
fig, ax = plt.subplots(nrows=1, ncols=2, figsize = (10, 10))
# Image without localization
ax[0].imshow((path_to_tensor(img_path).astype('float32')/255).squeeze())
# Image with localization
CAM = plot_CAM(img_path, ax[1], mobilenet_model, all_amp_layer_weights)
plt.show()

# predict_0_0 = 0.7350246
# predict_0_1 = 0.26497543

# Getting the image tensor
image_to_predict = path_to_tensor(img_path).astype('float32')/255

# Predicting the image
prediction = ensemble_model.predict(image_to_predict)
prediction_final = "Melanoma: " + str(np.round(predict_0_0*100, decimals = 4)) + "%" + \
                   " | Other illness: " + str(np.round(predict_0_1*100, decimals = 4)) + "%"

# Canvas initialization
fig = plt.figure(figsize = (10, 10))

# First image
ax = fig.add_subplot(121)
ax.imshow(image_to_predict.squeeze())
ax.text(0.3, 1.6, prediction_final)

# Second image
ax = fig.add_subplot(122)
CAM = plot_CAM(img_path, ax, mobilenet_model, all_amp_layer_weights)

plt.show()

"""## 9. Saving the Complete Model for the Python Interface Application"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/FINAL SAVED OUTPUTS/"
ensemble_model.save('FINAL_FILE_for_Soft_Computing_Project_Skin_Cancer.h5')
from tensorflow.keras.models import load_model
h5_saved_ensemble_model = load_model('FINAL_FILE_for_Soft_Computing_Project_Skin_Cancer.h5')
h5_saved_ensemble_model.summary()

"""## 10. Converting the .h5 File to .tflite File for the Python Interface Application"""

import tensorflow as tf

saved_ensemble_model = tf.keras.models.load_model('/content/drive/MyDrive/dataset/ISIC Challenge 2017 Organized/FINAL SAVED OUTPUTS/FINAL_FILE_for_Soft_Computing_Project_Skin_Cancer.h5')
converter = tf.lite.TFLiteConverter.from_keras_model(saved_ensemble_model)
tflite_model = converter.convert()
open("FINAL_FILE_for_Interface_Soft_Computing_Project_Skin_Cancer.tflite", "wb").write(tflite_model)